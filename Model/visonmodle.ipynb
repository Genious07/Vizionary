{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUEhZ8QmXvq7"
      },
      "outputs": [],
      "source": [
        "pip install -q torch torchvision timm accelerate torchvision==0.15.2\n",
        "pip install -q huggingface-hub gradio pytorch-grad-cam==1.4.6 matplotlib tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "foXjC3tIX2ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "import os\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "val_transform = transforms.Compose([transforms.Resize(224), transforms.ToTensor()])\n",
        "\n",
        "from torchvision.datasets import CIFAR10\n",
        "root = \"/content/data_cifar\"\n",
        "for split in [\"train\",\"val\"]:\n",
        "    os.makedirs(os.path.join(root, split, \"class_0\"), exist_ok=True)\n",
        "\n",
        "\n",
        "import torchvision.utils as vutils\n",
        "ds = CIFAR10(root=\"/content/cifar\", train=True, download=True)\n",
        "import PIL.Image as Image\n",
        "for i in range(200):\n",
        "    img, label = ds[i]\n",
        "    cls = f\"class_{label%2}\"\n",
        "    p = os.path.join(root, \"train\", cls, f\"{i}.png\")\n",
        "    os.makedirs(os.path.dirname(p), exist_ok=True)\n",
        "    img.save(p)\n",
        "ds2 = CIFAR10(root=\"/content/cifar\", train=False, download=True)\n",
        "for i in range(100):\n",
        "    img, label = ds2[i]\n",
        "    cls = f\"class_{label%2}\"\n",
        "    p = os.path.join(root, \"val\", cls, f\"{i}.png\")\n",
        "    os.makedirs(os.path.dirname(p), exist_ok=True)\n",
        "    img.save(p)\n",
        "\n",
        "print(\"Quick test dataset ready at\", root)\n"
      ],
      "metadata": {
        "id": "2c9BOWTGZrVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "\n",
        "DATA_DIR = \"\"\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "])\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "train_ds = datasets.ImageFolder(os.path.join(DATA_DIR, \"train\"), transform=train_transform)\n",
        "val_ds = datasets.ImageFolder(os.path.join(DATA_DIR, \"val\"), transform=val_transform)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "num_classes = len(train_ds.classes)\n",
        "print(\"Classes:\", train_ds.classes, \"Num classes:\", num_classes)\n"
      ],
      "metadata": {
        "id": "bZeM-q4tZ0o8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "import torch.nn as nn\n",
        "\n",
        "def create_model(num_classes):\n",
        "    model = timm.create_model('mobilenetv3_small_100', pretrained=False, num_classes=num_classes)\n",
        "    return model\n",
        "\n",
        "model = create_model(num_classes)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "mW-AHU9vZ6wO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "epochs = 8\n",
        "lr = 1e-3\n",
        "weight_decay = 1e-4\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "save_dir = \"/content/checkpoints\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    pbar = tqdm(loader, desc=\"train\")\n",
        "    for imgs, labels in pbar:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "        _, preds = outputs.max(1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "        pbar.set_postfix(loss=running_loss/total, acc=correct/total)\n",
        "    return running_loss/total, correct/total\n",
        "\n",
        "def validate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(loader, desc=\"val\")\n",
        "        for imgs, labels in pbar:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * imgs.size(0)\n",
        "            _, preds = outputs.max(1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            pbar.set_postfix(loss=running_loss/total, acc=correct/total)\n",
        "    return running_loss/total, correct/total\n",
        "\n",
        "best_acc = 0.0\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
        "    print(f\"Train: loss {train_loss:.4f}, acc {train_acc:.4f} | Val: loss {val_loss:.4f}, acc {val_acc:.4f}\")\n",
        "    # save checkpoint\n",
        "    ckpt = os.path.join(save_dir, f\"mobilenet_epoch{epoch+1}_valacc{val_acc:.4f}.pt\")\n",
        "    torch.save({'epoch': epoch+1, 'model_state_dict': model.state_dict(), 'val_acc': val_acc}, ckpt)\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), os.path.join(save_dir, \"best_model.pth\"))\n",
        "    # simple LR schedule\n",
        "    for g in optimizer.param_groups:\n",
        "        g['lr'] *= 0.98\n"
      ],
      "metadata": {
        "id": "U_D7QzxdaAtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
        "\n",
        "# load best model\n",
        "model.load_state_dict(torch.load(\"/content/checkpoints/best_model.pth\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "\n",
        "img_path = next(iter(val_ds.samples))[0]\n",
        "img = Image.open(img_path).convert(\"RGB\")\n",
        "img_resize = val_transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(img_resize)\n",
        "    probs = torch.nn.functional.softmax(logits, dim=1)[0].cpu().numpy()\n",
        "    pred = probs.argmax()\n",
        "print(\"Predicted:\", train_ds.classes[pred], \"Confidence:\", float(probs[pred]))\n",
        "\n",
        "# Grad-CAM\n",
        "target_layer = model.get_classifier() if hasattr(model, 'get_classifier') else model.blocks[-1]\n",
        "# safe pick for timm mobile net:\n",
        "try:\n",
        "    target_layer = model.conv_head\n",
        "except:\n",
        "    # fallback: use last features layer\n",
        "    for name, module in reversed(list(model.named_modules())):\n",
        "        if isinstance(module, torch.nn.Conv2d):\n",
        "            target_layer = module\n",
        "            break\n",
        "\n",
        "cam = GradCAM(model=model, target_layers=[target_layer], use_cuda=(device.type=='cuda'))\n",
        "rgb_img = np.array(img.resize((224,224))) / 255.0\n",
        "input_tensor = preprocess_image(rgb_img, mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225))\n",
        "grayscale_cam = cam(input_tensor=input_tensor, targets=None)[0]\n",
        "visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(img.resize((224,224)))\n",
        "plt.title(\"Input\")\n",
        "plt.axis('off')\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(visualization)\n",
        "plt.title(\"Grad-CAM\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7A7DlMYIaCJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "model_cpu = model.to('cpu').eval()\n",
        "\n",
        "example = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "traced = torch.jit.trace(model_cpu, example)\n",
        "torch.jit.save(traced, \"/content/checkpoints/mobilenet_traced.pt\")\n",
        "print(\"Saved TorchScript traced model.\")\n",
        "\n",
        "# Apply dynamic quantization on the traced model (only works on torch.nn.Linear, LSTM etc. but will reduce size)\n",
        "# To quantize the original model weights (state dict) use torch.quantization.quantize_dynamic\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model_cpu, {torch.nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "torch.jit.save(torch.jit.trace(quantized_model, example), \"/content/checkpoints/mobilenet_quantized_traced.pt\")\n",
        "print(\"Saved quantized TorchScript model.\")\n"
      ],
      "metadata": {
        "id": "UMO4cVcTaTi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# login\n",
        "huggingface-cli login\n"
      ],
      "metadata": {
        "id": "dh_hlkIXadSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, login, create_repo, upload_file\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(token=\"\")\n",
        "\n",
        "api = HfApi()\n",
        "repo_id = \"\"   # change\n",
        "api.create_repo(repo_id=repo_id, repo_type=\"model\", exist_ok=True)\n",
        "\n",
        "# Upload artifacts\n",
        "api.upload_file(path_or_fileobj=\"/content/checkpoints/mobilenet_traced.pt\",\n",
        "                path_in_repo=\"mobilenet_traced.pt\",\n",
        "                repo_id=repo_id)\n",
        "api.upload_file(path_or_fileobj=\"/content/checkpoints/mobilenet_quantized_traced.pt\",\n",
        "                path_in_repo=\"mobilenet_quantized_traced.pt\",\n",
        "                repo_id=repo_id)\n",
        "\n",
        "\n",
        "api.create_repo(repo_id=\"\", repo_type=\"space\", exist_ok=True)\n",
        "from huggingface_hub import Repository\n",
        "repo = Repository(local_dir=\"/content/space_repo\", clone_from=\"your-username/vision-demo\")\n",
        "repo.push_to_hub(commit_message=\"Initial Gradio demo\")\n"
      ],
      "metadata": {
        "id": "wDafBN_Hagr8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}